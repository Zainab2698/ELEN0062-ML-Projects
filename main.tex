\documentclass[acmconf,nonacm=true]{acmart}
\authorsaddresses{}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{soul}
\usepackage{cite}
\usepackage{float}
\sethlcolor{yellow} 
\usepackage{amsmath}
\begin{document}

\title{ELEN0062 - Introduction to Machine Learning}
\subtitle{Project 1 - Classification Algorithms}

% Enter your names here, along with your student IDs.
\author{FAINGNAERT Th√©o (\texttt{s191662})}
\author{ZIANI Zaynab (\texttt{s2403973})}
\author{Author RULMONT Quentin (\texttt{s194355})}

\maketitle

\section{Decision Trees}


\textbf{1. Observe how the decision boundary is affected by tree complexity}\\
\textbf{i. Illustration for each depth value can be found in the appendix}
\newline
    \textbf{ii. Explanation of the decision boundary for each hyperparameter value.}
    \begin{itemize}
        \item \textbf{Depth = 1: }The decision boundary is a simple straight line because the decision tree makes an axis-aligned decision, dividing the data space in 2 regions. The classification is oversimplified and most likely underfitted.
    \end{itemize}
    \begin{itemize}
        \item \textbf{Depth = 2 :}  With this 1 additional split, the model is more flexible, however, it remains relatively still linear and simplistic.
    \end{itemize}
   \begin{itemize}
       \item \textbf{ Depth = 4: }At for levels of splits, the decision boundary becomes non-linear, resulting in the decision tree adapting more to the structure of the data.
   \end{itemize}
   \begin{itemize}
       \item \textbf{Depth = 8: } In this case the decision boundaries are very detailed and complex, we see (in the figure) that it adapts to "almost" every detail of the data even the small variations or ouliers in the data set.
       \item \textbf{Depth = None: } The model does not classify the data as expected, but over-fitting and failing to recognize some of the orange dots as orange, and same goes for some of the blue dots infiltrated amongst the orange ones
    \end{itemize}
  \textbf{b. Discuss when the model is clearly underfitting/overfitting and detail your evidence for each claim.}
    \begin{itemize}
        \item When the model is too simple (depth = 1) the decision boundary doesn't adapt well to the data and the accuracy on both training and testing data sets is low which idicates underfitting.
    \item At max-depth = none, the test accuracy is not optimal, even though the decision boundary is complex.
    \end{itemize}
\textbf{c. Explain why the model is more confident when max\_depth is the largest} \\
    The model is more confident as it over-fits the \textit{training data}, assigning each data point to a pure leaf that classifies it. This confidence however, as it fits perfectly to the training data won't adapt well enough to a new one (testing set) and the model will probably learn not relevant nuances (noise).  
\\
    \textbf{2. Report the average test set accuracies over five generations of the dataset,
    along with the standard deviations, for each value of depth.}\\
The low standard deviation shows that the accuracy is consistent between each generation.\\
As the max depth value increases, the decision boundary becomes more flexible and accuracy improves up to a level, after which, further depths might lead to test accuracies decreasing slightly. (under to over-fitting)\\

\begin{table}[H]
    \centering
    \caption{Accuracy and Standard Deviation of Decision Trees}
    \begin{tabular}{|c|c|c|}
    \hline
     max\_depth & Average Accuracy & Standard Deviation \\
    \hline
    1 & 85.59\%&  0.0141\\
    2 & 89.57\%&  0.0094\\
    4 & 91.72\%&  0.0029\\
    8 & 89.27\%&  0.0098\\
    None & 88.58\%&  0.0130\\
    \hline
    \end{tabular}
\end{table}

\section{k-Nearest Neighbors}
\textbf{1. Observe how the decision boundary is affected by the number of neighbors.}
    \begin{enumerate}
    \item See appendix for figures.
    \item 
        \begin{enumerate}
            \item When \textbf{n\_neighbors = 1}, the decision boundary illustrates a classical case of over-fitting. The algorithm has learned every noise data, creating islands in the plot. For $n\_neighbors = 5$, islands are still fairly visible.
            \item For \textbf{n\_neighbors = 50\ \&\ 100}, the decision boundary is acceptable. It follows a smooth path between the two groups.
        \end{enumerate}
    \item For \textbf{n\_neighbors = 500}, the algorithm is clearly under-fitting. The class fails to predict the blue dots because they are a minority, thus it will most likely find more orange dots as neighbors. This effect can also be explained by the fact that when $n$ approaches the training size (1000 in our case), the confidence will converge toward the orange dots' proportion.
    \end{enumerate}
\textbf{2. Report the average test set accuracies over five generations of the dataset,
along with the standard deviations, for each value of n.}\\
The low standard deviation shows that the accuracy is consistent between each generation.\\
The variance-bias tradeoff is clearly visible, with the accuracy increasing with $n$ before decreasing at $n=100$. The class is overfitting at $n=1$ and underfitting at $n=500$.

    \begin{table}[H]
    \centering
    \caption{Accuracy and Standard Deviation of k-Nearest Neighbors}
    \begin{tabular}{|c|c|c|}
    \hline
     neighbors & Average Accuracy & Standard Deviation \\
    \hline
    1 & 88.82\% & 0.0114 \\
    5 & 91.90\% & 0.0048 \\
    50 & 92.66\% & 0.0027 \\
    100 & 92.60\% & 0.0024 \\
    500 & 80.41\% & 0.0567 \\
    \hline
    \end{tabular}
    \end{table}

\section{Perceptron}
\textbf{1) Derive the mathematical expression of the gradient}
The loss function is defined as:
$$
\mathcal{L}(\mathbf{x}, y, \mathbf{w})=-y \log (\hat{f}(\mathbf{x} ; \mathbf{w}))-(1-y) \log (1-\hat{f}(\mathbf{x} ; \mathbf{w}))
$$
Where
$$\hat{f}(\mathbf{x} ; \mathbf{w}) = \sigma(w_0 + \sum^{p}_{j=1}w_j x_j)$$
and $\sigma(x) = \frac{1}{1+e^{-x}}$ is a sigmoid. In this case, p = 2.\\
Deriving $\mathcal{L}(\mathbf{x}, y, \mathbf{w})$, we obtain:
\begin{align}
    \frac{\partial \mathcal{L}}{\partial w_0} &= \frac{-y}{\sigma(w_0 + \sum^{p}_{j=1}w_j x_j)}\sigma'(w_0 + \sum^{p}_{j=1}w_j x_j) + \frac{-y}{1-(\sigma(w_0 + \sum^{p}_{j=1}w_j x_j))}\sigma'(w_0 + \sum^{p}_{j=1}w_j x_j)\\
    \frac{\partial \mathcal{L}}{\partial w_1} &=\frac{-y}{\sigma(w_0 + \sum^{p}_{j=1}w_j x_j)}\sigma'(w_0 + \sum^{p}_{j=1}w_j x_j) x_0+ \frac{-y}{1-(\sigma(w_0 + \sum^{p}_{j=1}w_j x_j))}\sigma'(w_0 + \sum^{p}_{j=1}w_j x_j) x_0\\ 
    \frac{\partial \mathcal{L}}{\partial w_2} &=\frac{-y}{\sigma(w_0 + \sum^{p}_{j=1}w_j x_j)}\sigma'(w_0 + \sum^{p}_{j=1}w_j x_j) x_1+ \frac{-y}{1-(\sigma(w_0 + \sum^{p}_{j=1}w_j x_j))}\sigma'(w_0 + \sum^{p}_{j=1}w_j x_j) x_1
\end{align}

Knowing that $\sigma'(x) = \sigma (x)\cdot (1-\sigma(x))$, the equation can be simplified

\begin{align}
    \frac{\partial \mathcal{L}}{\partial w_0} &= -y + 2\sigma(w_0 + \sum^{p}_{j=1}w_j x_j)-y\sigma(w_0 + \sum^{p}_{j=1}w_j x_j)\\
    \frac{\partial \mathcal{L}}{\partial w_1} &=x_0(-y + 2\sigma(w_0 + \sum^{p}_{j=1}w_j x_j)-y\sigma(w_0 + \sum^{p}_{j=1}w_j x_j))\\
    \frac{\partial \mathcal{L}}{\partial w_2} &=x_1(-y + 2\sigma(w_0 + \sum^{p}_{j=1}w_j x_j)-y\sigma(w_0 + \sum^{p}_{j=1}w_j x_j))
\end{align}
\textbf{2) Briefly explain and motivate your implementation of the perceptron}
The implementation of the perceptron predictor follows the convention of the scikit learn library. Regarding the detail, the initialisation of the weight vector is done at random. Note that to have reproducible result, the initial vector is fixed in the code. But in practice we will use a random generator. This will allows the algorithm to reach a better result faster and not introduce some bias. For the training, we use epoch of size 5. Using epoch instead of a single data to update the weight allows our model to change less with outliers since they will be mitigated by the remaining date in that epoch. It is a way to limit overfitting.

\textbf{1. Observe how the decision boundary is affected by the learning rate.}
    \begin{enumerate}
    \item See appendix for figures.
    \item In general, for each value of $\eta$, we observe the same result. The main difference is the angle of the boundary between the two classes. However a huge difference is visible When the learning rate is to low, we can see that the prediction are very wrong. It comes from the fact that the weight vector is initialise with a big weight for the last component. It means that there is not enough data to converge to a good model given a small learning rate. We can see on the graph that starting from eta = 0.01, the result shift and predict correctly the data. From the decision boundary, we can see that the perceptron create a linear model. Furthermore, from the two last figures, we see that increasing the learning rate, decrease the uncertainty area(in white). It means that it start overfiting the data.
    \end{enumerate}

\textbf{2. Report the average test set accuracies over five generations of the dataset,
along with the standard deviations, for each value of $\eta$.}\\

As explain earlier, the model behave poorly for small learning rate but is rather good for bigger learning rate. 
We can also see from the standard deviation that the best value seams to be a learning rate of 0.01 because it has the smallest variance after that, the variance increase which means the model start to overfit.

\begin{table}[H]
    \centering
    \caption{Accuracy and Standard Deviation of the perceptron}
    \begin{tabular}{|c|c|c|}
    \hline
     learning rate($\eta$) & Average Accuracy & Standard Deviation \\
    \hline
    0.0001 & 8,61\% & 0.0016 \\
    0.0005 & 9,21\% & 0.0026 \\
    0.001 & 11,3\% & 0.0077 \\
    0.01 & 92.8\% & 0.0015 \\
    0.1 & 92.37\% & 0.0059 \\
    \hline
    \end{tabular}
    \end{table}
\section{Method comparison}
\begin{enumerate}
    \item Cross-validation is a robust method for tuning hyperparameters. \\
     In this context, max\_depth for Decision Trees, n\_neighbors for K-Nearest Neighbors, and $\eta$ for Perceptron,\\
     it allows us to make use of the learning set efficiently.
     By averaging the model's performance across multiple folds, we reduce the risk of overfitting to a particular subset of the data, making sure the chosen hyperparameters adapt well to out-of-sample data.\\
     Cross-Validation Process:
    	We would split the learning set into k-folds (commonly k=5 or k=10).
    	For each hyperparameter setting, the model will be trained on k-1 folds and validated on the remaining fold.
    	This process is repeated k times, with a different fold being used for validation each time.
    	The average performance (e.g., accuracy) across the k validation rounds will be calculated, and the hyperparameter setting that produces the best average performance will be selected.
    \item
\begin{enumerate}
    \item \textbf{Decision Tree}:\\
    Optimal results are obtained when the hyperparameter max depth of the decision tree reaches 4.\\
    The standard deviation is low at 0.00298, which shows a consistency of performance that gets a litte bit distorted when the 200 data points are added and the standard deviation increases to 0.0074.\\
    Adding irrelevant features in general to a model makes the latter shallower and reduces down accuracy. However, in the case of the decision tree, even with the noise, the performance of the model remains robust and the accuracy only dropped moderately.\\
    The optimal max depth with 200 irrelevant features (noise) introduced to the dataset is 2. the reason why the depth of processing isn't as deep, is to prevent the model from overfitting. The deeper the tree would go, the more it would attempt to fit to the noisy patterns implying a deterioration in performance.
    \item \textbf{K-nearest neighbors} displays only a 0.3\% loss of accuracy.\\
    This is because Knn is based on average distance thus the likelihood that noisy points consistently dominate the k-nearest neighbors of any given point is low.\\
    Also, we can see the parameter $n$ lowering to 11.\\
    The effect of the noisy points diminishes with $n$ diminishing because they are likely to be outnumbered by correctly labeled data in the majority of neighborhoods.\\
    Knn is high in ranking in case of noisy data.
    \item \textbf{Perceptron}:
    In the case of the perceptron, we can see that it is the best model in both cases in terms of accuracy. It is expected as the perceptron fit a linear model on our data and in the case of 2 Gaussians, the best model is linear. However when regarding the variance, knn is better. It can be explain by the fact that knn make an average of its neighbors and thus is more stable with less variance. Regarding the effect of adding 200 irrelevant data, we can see that it changes the best learning rate computed by cross validation an decrease the quality of the model. It was expected as adding noise to our training data will surely make our model learn bad examples and thus decrease the efficiency. 

\end{enumerate}
\begin{table}[H]
    \centering
    \caption{Hyperparameter, Accuracy and Standard Deviation of methods}
    \begin{tabular}{|c||c|c|c||c|c|c|}
    \hline
    \multicolumn{1}{|c||}{} &\multicolumn{3}{c||}{No noise} & \multicolumn{3}{c|}{Noisy} \\
    \hline
    Method & Tuned Parameter & Average Accuracy & Standard Deviation & Tuned Parameter & Average Accuracy & Standard Deviation\\
    \hline
    dt & 4 & 91.67\% & 0.0029 & 2 & 89.79\% & 0.0074\\
    knn & n=44 & 92.6\% & 0.0012 & n=11 & 92.3\% & 0.0018\\
    perceptron & $\eta$ = 0.01 &  92.8\% & 0.0015 & $\eta$ = 0.1 & 92.37\% & 0.0059\\
    \hline
    \end{tabular}
\end{table}

\end{enumerate}


\section{Appendix}
\begin{figure}[H]
    \centering  
    \includegraphics[width=0.75\linewidth]{dt/decision_boundary_depth_1.pdf}
    \caption{max\_depth = 1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{dt/decision_boundary_depth_2.pdf}
    \caption{max\_depth = 2}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{dt/decision_boundary_depth_4.pdf}
    \caption{max\_depth = 4}
\end{figure}

\begin{figure}[H]
    \centering  
    \includegraphics[width=0.75\linewidth]{dt/decision_boundary_depth_8.pdf}
    \caption{max\_depth = 8}
\end{figure}

\begin{figure}[H]
    \centering  
    \includegraphics[width=0.75\linewidth]{dt/decision_boundary_depth_None.pdf}
    \caption{max\_depth = None}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{knn/knn_1.pdf}
    \caption{n = 1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{knn/knn_5.pdf}
    \caption{n = 5}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{knn/knn_50.pdf}
    \caption{n = 50}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{knn/knn_100.pdf}
    \caption{n = 100}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{knn/knn_500.pdf}
    \caption{n = 500}
\end{figure}

\begin{figure}[H]
    \centering  
    \includegraphics[width=0.75\linewidth]{perceptron/perceptron_learning_rate=0.0001.pdf}
    \caption{learning rate ($\eta$) = 0.0001}
\end{figure}

\begin{figure}[H]
    \centering  
    \includegraphics[width=0.75\linewidth]{perceptron/perceptron_learning_rate=0.0005.pdf}
    \caption{learning rate ($\eta$) = 0.0005}
\end{figure}

\begin{figure}[H]
    \centering  
    \includegraphics[width=0.75\linewidth]{perceptron/perceptron_learning_rate=0.0010.pdf}
    \caption{learning rate ($\eta$) = 0.001}
\end{figure}

\begin{figure}[H]
    \centering  
    \includegraphics[width=0.75\linewidth]{perceptron/perceptron_learning_rate=0.0100.pdf}
    \caption{learning rate ($\eta$) = 0.01}
\end{figure}

\begin{figure}[H]
    \centering  
    \includegraphics[width=0.75\linewidth]{perceptron/perceptron_learning_rate=0.1000.pdf}
    \caption{learning rate ($\eta$) = 0.1}
\end{figure}



\end{document}